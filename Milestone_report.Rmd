---
title: "Johns Hopkins Data Science Specialization : Milestone Report"
subtitle : "Exploratory Data Analysis and Modeling"
author : "Suhas. P. K"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "dark"
    downcute_theme: "chaos"
    
---


```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
```

## Project Overview

### Introduction

Introduction to mobile devices led many inventions and innovations. Once there was a time where sending an email was a luxury. Within a span of two decades the evolution of communication devices from a huge hefty computing machines to a simple palm held, touch responsive mobile phone is observed significantly.

One of the basic features of these mobile devices is to predicts words based on the sentences that the user types.

In this **milestone project**, with the help of **SwiftKey** and **Johns Hopkins Data Science Specialization Course**, I will be attempting to build a predictive text models like those used by SwiftKey.

In this project includes analyzing a large corpus of text documents to discover the structure in the data, and how words are put
together. This involves cleaning and analysisng text data, building and sampling predictive text model. And build a predictive text product using Rshiny.

### Resources 

- [Text mining infrastructure in R](https://www.jstatsoft.org/article/view/v025i05)
- [CRAN Task View: Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)
- [NLP slides](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html)
-[NLP Wiki](https://en.wikipedia.org/wiki/Natural_language_processing)

### Course datasets

This is the training dataset that I will be using for the basic exercises of this project.

[Download here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

## Game Plan 

The libraries used are `knitr, stringi,NLP,tm,RWeka,SnowballC, data.table, wordcloud, ggplot2, kableExtra, gridExtra, ggdark, and RColorBrewer`.
### Libraries

```{r libraries, message=FALSE, echo=FALSE, warning=FALSE}
if(!require(knitr)){
  install.packages("knitr")
  library(knitr)
}
if(!require(stringi)){
  install.packages("stringi")
  library(stringi)
}
if(!require(NLP)){
  install.packages("NLP")
  library(NLP)
}
if(!require(tm)){
  install.packages("tm")
  library(tm)
}
if(!require(RWeka)){
  install.packages("RWeka")
  library(RWeka)
}
if(!require(data.table)){
  install.packages("data.table")
  library(data.table)
}
if(!require(ggplot2)){
  install.packages("ggplot2")
  library(ggplot2)
}
if(!require(SnowballC)){
  install.packages("SnowballC")
  library(SnowballC)
}
if(!require(wordcloud)){
  install.packages("wordcloud")
  library(wordcloud)
}
if(!require(kableExtra)){
  install.packages("kableExtra")
  library(kableExtra)
}
if(!require(gridExtra)){
  install.packages("gridExtra")
  library(gridExtra)
}
if(!require(ggdark)){
  install.packages("ggdark")
  library(ggdark)
}
if(!require(RColorBrewer)){
  install.packages("RColorBrewer")
  library(RColorBrewer)
}
```

### Importing the datasets.

```{r import datasets,message=FALSE}

# Read Blog lines
blogs <- file(
  paste(getwd(),'Coursera-SwiftKey/final/en_US/en_US.blogs.txt',sep = "/"),
  "r"
)
blog_lines <- readLines(blogs, warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
close(blogs)

# Read News lines
news <- file(
  paste(getwd(),'Coursera-SwiftKey/final/en_US/en_US.news.txt',sep = "/"),
  "r"
)
news_lines <- readLines(news,warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
close(news)

# Read Twitter Lines
twitter <- file(
  paste(getwd(),'Coursera-SwiftKey/final/en_US/en_US.twitter.txt',sep = "/"),
  "r"
)
twitter_lines <- readLines(twitter,warn = FALSE, encoding = "UTF-8", skipNul = TRUE)
close(twitter)


```

### General statistics on 'Words per line (wpl)'

```{r wpl, message=FALSE}

# number of lines 
num_lines <- sapply(list(blog_lines,news_lines,twitter_lines), length)

# number of characters
num_char <- sapply(list(nchar(blog_lines),nchar(news_lines),nchar(twitter_lines)),sum)

# number of words 
num_words <- sapply(list(blog_lines,news_lines,twitter_lines), stri_stats_latex)[4,]

# number of words per line
wpl <- lapply(list(blog_lines,news_lines,twitter_lines), function(x){stri_count_words(x)})

# summary 
wpl_gist <- sapply(list(blog_lines,news_lines,twitter_lines), function(x){
  summary(stri_count_words(x))[c('Min.', 'Mean', 'Max.')]})

rownames(wpl_gist) = c('wpl_min', 'wpl_mean', 'wpl_max')

gist <- data.frame(
  FileName = c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"),
  FileSize = sapply(
    list(blog_lines,news_lines,twitter_lines), function(x){format(object.size(x),"MB")}
  ),
  Lines = num_lines,
  Charaters = num_char,
  Words = num_words,
  t(rbind(round(wpl_gist)))
  
)

# Tabular representation

gist %>% kbl() %>%  kable_styling(bootstrap_options = c("condensed", "responsive"))

```



```{r wpl_histogram, message=FALSE, warning=FALSE}
plot1 <- qplot(wpl[[1]],
               geom = "histogram",
               main = "US Blogs",
               xlim = c(0,200),
               xlab = "Words per Line",
               ylab = "Frequency",
               binwidth = 5) + dark_theme_light()

plot2 <- qplot(wpl[[2]],
               geom = "histogram",
               main = "US News",
               xlim = c(0,300),
               xlab = "Words per Line",
               ylab = "Frequency",
               binwidth = 5) + dark_theme_light()

plot3 <- qplot(wpl[[3]],
               geom = "histogram",
               main = "US Twitter",
               xlab = "Words per Line",
               ylab = "Frequency",
               binwidth = 1) + dark_theme_light()

plotList = list(plot1, plot2, plot3)
do.call(grid.arrange, c(plotList, list(ncol = 1)))

# free up some memory
rm(plot1, plot2, plot3)
```

### Sampling and cleaning.
```{r sampling and cleaning, message=FALSE}
set.seed(660067)

# assigning sample size
sampleSize = 0.01

# sampling all 3 datasets
sample_blogs <- sample(blog_lines, length(blog_lines)*sampleSize, replace = FALSE)
sample_news <- sample(news_lines, length(blog_lines)*sampleSize, replace = FALSE)
sample_twitter <- sample(twitter_lines, length(blog_lines)*sampleSize, replace = FALSE)

# Removing all non-English characters from the sample data.

sample_blogs <- iconv(sample_blogs, "latin1", "ASCII", sub="")
sample_news <- iconv(sample_news, "latin1", "ASCII", sub="")
sample_twitter <- iconv(sample_twitter, "latin1", "ASCII", sub="")

# Combining all 3 sample data sets into a single data set and writing to disk.

sample_data <- c(sample_blogs,sample_news,sample_twitter)
sample_data_filename <- paste(getwd(),                          "Coursera-SwiftKey/final/en_US/en_US.sample.txt",sep = "/")
con <- file(sample_data_filename, open = "w")
writeLines(sample_data,con)
close(con)

# number of lines and words in the sample data set.
sample_data_lines <- length(sample_data)
sample_data_words <- sum(stri_count_words(sample_data))
sample_data_lines


```

### Profanity dataset.

- For **profanity check** I downloaded a .csv file from **Kaggle** [Profanities in English](https://www.kaggle.com/datasets/konradb/profanities-in-english-collection). 
- The downloaded data set is in csv format. So I changed it to a text format so all data set are in same format.
- It is saved as 'en_US.profanity.txt' .

### Building Corpus

```{r corpus, message=FALSE, warning=FALSE}

build_corpus <- function(dataset){
  docs <- VCorpus(VectorSource(dataset))
  to_space <- content_transformer(
    function(x,pattern)
      gsub(pattern, " ",x)
  )
  
  # Removing URL, Twitter handles, email pattern.
  docs <- tm_map(docs, to_space,"(f|ht)tp(s?)://(.*)[.][a-z]+")
  docs <- tm_map(docs, to_space,"@[^\\s]+")
  docs <- tm_map(docs, to_space, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b")
  
  # Removing Profane words from the sample data.
  
  con <- file(
    paste(getwd(),'Coursera-SwiftKey/final/en_US/en_US.profanity.txt',sep = "/"),
  "r"
  )
  profanity <-readLines(
    con, warn = FALSE, encoding = "UTF-8", skipNul = TRUE
  )
  close(con)
  
  profanity <- iconv(profanity, "latin1", "ASCII", sub = "")
  docs <- tm_map(docs, removeWords,profanity)
  
  # Converting the words to lower case.
  docs <- tm_map(docs, tolower)
  # Removing stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  # Removing punctuation marks.
  docs <- tm_map(docs, removePunctuation)
  # Removing white spaces.
  docs <- tm_map(docs, stripWhitespace)
  # Removing Plain text document
  docs <- tm_map(docs, PlainTextDocument)
  
  return(docs)
}

# Building corpus and writing to disk

corpus <- build_corpus(sample_data)
saveRDS(corpus, file = paste(
  getwd(),"Coursera-SwiftKey/final/en_US/en_US.corpus.rds", sep = "/"
))

# Converting the corpus to a data frame and writing the lines/word to disks.

corpus_text <- data.frame(text = unlist(
  sapply(corpus, '[', "content")), stringsAsFactors = FALSE
)

corpus_filename <- paste(getwd(),"Coursera-SwiftKey/final/en_US/en_US.corpus.txt",sep = "/")
con <- file(corpus_filename, open = "w")
writeLines(corpus_text$text,con)
close(con)



```
### Word Frequencies

```{r word frequency, message=FALSE, warning=FALSE}

tdm <- TermDocumentMatrix(corpus)
freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
word_Freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)


# Plotting top 10 most frequent words.

plot_most_freq_word <- ggplot(
  word_Freq[1:10,], aes(x = reorder(word_Freq[1:10,]$word, -word_Freq[1:10,]$freq),
                       y = word_Freq[1:10,]$freq)
)

plot_most_freq_word <- plot_most_freq_word+
  geom_bar(stat = "Identity", fill = "#90EE90" ) + 
  geom_text(aes(label = word_Freq[1:10,]$freq), vjust = -0.20, size=3) + xlab("Words")+ ylab("Word Frequencies")+theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 0.5, vjust = 0.5, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))+ggtitle("10 Most Frequent Words")+dark_theme_light()

plot_most_freq_word

# Word cloud
suppressWarnings(
  wordcloud(
    words = word_Freq$word,
    freq = word_Freq$freq,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(10, "Dark2")
  ) 
  
  
)

```

### Tokenizing and N-Gram Generation
```{r tokenization, message=FALSE, warning=FALSE}

unigram_tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=1, max = 1))
}

bigram_tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=2,max=2))
}

trigram_tokenizer <- function(x){
  NGramTokenizer(x,
                 Weka_control(min=3, max=3))
}
```

### Unigrams
```{r unigrams, message=FALSE, warning=FALSE}

# creating term document matrix for the corpus
unigram_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = unigram_tokenizer))

# Eliminating sparse terms for each N-gram and get frequencies of most common n-grams

unigram_matrix_freq <- sort(rowSums(as.matrix(
  removeSparseTerms(
    unigram_matrix, 0.99
  )
)), decreasing = TRUE)


unigram_matrix_freq <- data.frame(
  word = names(unigram_matrix_freq),
  freq = unigram_matrix_freq,
  row.names = NULL
)


# Plotting histogram
unigram_hist <- ggplot(
  unigram_matrix_freq[1:20,],
  aes(x = reorder(word, -freq), y = freq)
) + geom_bar(stat = "identity", fill = "#37BEB0")+ geom_text(
  aes(label = freq), vjust = -0.20, size = 3) + xlab("Words") + ylab("Frequency")+
  theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))+ ggtitle("20 Most Common Unigrams")+dark_theme_light()

unigram_hist

# Word cloud
suppressWarnings(
  wordcloud(
    words = unigram_matrix_freq$word,
    freq = unigram_matrix_freq$freq,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(10, "Dark2")
  ) 
  
  
)
```

### Bigrams
```{r bigram, message=FALSE, warning=FALSE}

# creating term document matrix for the corpus
bigram_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = bigram_tokenizer))

# Eliminating sparse terms for each N-gram and get frequencies of most common n-grams

bigram_matrix_freq <- sort(rowSums(as.matrix(
  removeSparseTerms(
    bigram_matrix, 0.999
  )
)), decreasing = TRUE)


bigram_matrix_freq <- data.frame(
  word = names(bigram_matrix_freq),
  freq = bigram_matrix_freq,
  row.names = NULL
)


# Plotting histogram
bigram_hist <- ggplot(
  bigram_matrix_freq[1:20,],
  aes(x = reorder(word, -freq), y = freq)
) + geom_bar(stat = "identity", fill = "#FFD580")+ geom_text(
  aes(label = freq), vjust = -0.20, size = 3) + xlab("Words") + ylab("Frequency")+
  theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))+ ggtitle("20 Most Common bigrams")+dark_theme_light()

bigram_hist

# Word cloud
suppressWarnings(
  wordcloud(
    words = bigram_matrix_freq$word,
    freq = bigram_matrix_freq$freq,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(10, "Dark2")
  ) 
  
  
)

```


### Trigrams

```{r trigrams, message=FALSE, warning=FALSE}
# creating term document matrix for the corpus
trigram_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = trigram_tokenizer))

# Eliminating sparse terms for each N-gram and get frequencies of most common n-grams

trigram_matrix_freq <- sort(rowSums(as.matrix(
  removeSparseTerms(
    trigram_matrix, 0.9999
  )
)), decreasing = TRUE)


trigram_matrix_freq <- data.frame(
  word = names(trigram_matrix_freq),
  freq = trigram_matrix_freq,
  row.names = NULL
)



# Plotting histogram
trigram_hist <- ggplot(
  trigram_matrix_freq[1:20,],
  aes(x = reorder(word, -freq), y = freq)
) + geom_bar(stat = "identity", fill = "#fff44f")+ geom_text(
  aes(label = freq), vjust = -0.20, size = 3) + xlab("Words") + ylab("Frequency")+
  theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 1.0, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))+ ggtitle("20 Most Common trigrams")+dark_theme_light()

trigram_hist

# Word cloud
suppressWarnings(
  wordcloud(
    words = trigram_matrix_freq$word,
    freq = trigram_matrix_freq$freq,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(10, "Dark2")
  ) 
  
  
)

```







